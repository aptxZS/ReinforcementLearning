% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage{listings}
\usepackage{color}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
%  frame=single,
  breaklines=true,
}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Reinforcement learning - Coursework}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Dimitri Diomaiuta - 30598109}
%
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Southampton}
%% \institute{Princeton University, Princeton NJ 08544, USA \and
%% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%% \email{lncs@springer.com}\\
%% \url{http://www.springer.com/gp/computer-science/lncs} \and
%% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
%% \begin{abstract}
%% Searching is one of the oldest artificial intelligence techniques used for problem solving. In this paper we analyze the results and scalability of both uninformed and informed search algorithms.
%% %\keywords{First keyword  \and Second keyword \and Another keyword.}
%% \end{abstract}
%
%
%

\section{Introduction}
The aim of this project is to develop algorithmic strategies to play
the Lemonade Stand Game (LSG). The LSG is a three agents game with a
12 positions setting, distributed like the hours on a clock.  At every
round, each player chooses a spot to put his stand on, $A_i = {P_1, ...,
  P_{12}}$. Every player chooses at the same time, without knowing the
choice of the other players at the current round since communication
between agents is not allowed. After each round, the utility of a
player is calculated as the sum of the distances between the closest
clockwise player and the closest anticlockwise player. The only
exception to this is when two or all players are sitting on the same
spot. In the first case the two players sitting in the same spot
collect a quarter of the total revenue of a single round while the
third collects half of it. In the second case each agent collects a
third of the total revenue. The total points given at each round depend
on how much points are assigned for a single spot. We use the 
$U_{spot}$ variable to indicate this measure and $R_{total}$ to
indicate the total revenue of a single round. Equation \ref{points} shows
how the total number of points for a round are calculated.
Equation \ref{points} multiplies the total number of points that the
spots generate by two since every point is collected by two
agents. Assigning $1$ to the unit spot variable leads to a total
revenue per round of $24$, while assigning $6$ to the spot variable
leads to a $144$ total revenue.
\begin{equation}\label{points}
R_{total} = U_{spot} \times positions \times 2
\end{equation}
The aim of each agent is, hence, to be as far as possible from the
other players, in order to maximize his individual total revenue for the n rounds
played. In this paper, we describe and evaluate different algorithmic
strategies to play the Lemonade Stand Game.

\section{Algorithm design}
In this section we describe the design of the implemented
algorithms. This section is divided into two subsection, each
describing a different class of algorithms. 

\subsection{Game agnostic algorithms}
This class of algorithms include strategies that reduce the game
to a multi-armed bandit setting. These algorithms are based on
evaluative feedback of the actions taken and do not have any internal
representation of the game or its rules. Given an array
of actions, the value of each of them, $q_*(a)$, at time step $t$ is the expected
reward $R_t$ returned when that action is taken, as equation \ref{eq2}
shows \cite{rlbook}:
\begin{equation}\label{eq2}
  q_*(a) \doteq \mathbb{E}(R_t | A_t = a)
\end{equation}
The aim of a multi-armed bandit algorithm is to derive an estimated
value for each action at time step $t$, $Q_t(a)$, as close as possible
to the real value $q_*(a)$. Exploration and exploitation techniques
are fundamental for this class of algorithms. The former allows to
obtain more accurate actions estimators. The latter allows to maximize
the reward by exploiting the current knowledge state.

\subsubsection{Exp3 algorithm}
The Exp3 algorithm implements an instance of the multi-armed bandit
algorithmic class. Given K actions, the Exp3 algorithm keeps track of
K-length weight and probability vectors, initialized respectively at 1
and $1/K$. The probability vector
assigns a probability to each action that is calculated according to
the weight given to that action. Actions with higher weights obtain an
higher probability. At every turn the action is selected according to
the probability vector. The reward of the selected action is then used
to update the weight of that action by exploiting the exponential
distribution. The exponential distribution increments the weights of
the highest-reward actions guaranteeing exploitation. On the other hand,
exploration is guaranteed by the $\gamma$ parameter which describe the
probability to draw a uniform random action when selecting it.

\subsubsection{Epsilon greedy algorithm}

\subsection{Heuristic based algorithms}
\subsubsection{Stick and follow algorithm}
\subsubsection{EA2 algorithm}


\section{Evaluation}

\section{Conclusion}

\begin{thebibliography}{8}

\bibitem{rlbook}
Sutton, R.S. and Barto, A.G., 2011. Reinforcement learning: An introduction.

\end{thebibliography}

\section{Appendix A: source code}\label{appendix}
This appendix section contains the source code of the implemented program.

%% \subsection{capture\_the\_flag.py}\label{capture_the_flag.py}
%% \lstinputlisting[language=Python]{../capture_the_flag.py}


\end{document}
