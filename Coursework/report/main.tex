% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage{listings}
\usepackage{color}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
%  frame=single,
  breaklines=true,
}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Reinforcement and Online Learning \; Coursework}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Dimitri Diomaiuta - 30598109}
%
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Southampton}
%% \institute{Princeton University, Princeton NJ 08544, USA \and
%% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%% \email{lncs@springer.com}\\
%% \url{http://www.springer.com/gp/computer-science/lncs} \and
%% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
%% \begin{abstract}
%% Searching is one of the oldest artificial intelligence techniques used for problem solving. In this paper we analyze the results and scalability of both uninformed and informed search algorithms.
%% %\keywords{First keyword  \and Second keyword \and Another keyword.}
%% \end{abstract}
%
%
%

\section{Introduction}
The aim of this project is to develop algorithmic strategies to play
the Lemonade Stand Game (LSG). The LSG is a three agents game with a
12 positions setting, distributed like the hours on a clock.  At every
round, each player chooses a spot to put his stand on, $A_i = {P_1, ...,
  P_{12}}$. Every player chooses at the same time, without knowing the
choice of the other players at the current round since communication
between agents is not allowed. After each round, the utility of a
player is calculated as the sum of the distances between the closest
clockwise player and the closest anticlockwise player. The only
exception to this is when two or all players are sitting on the same
spot. In the first case the two players sitting in the same spot
collect a quarter of the total revenue of a single round while the
third collects half of it. In the second case each agent collects a
third of the total revenue. The total points given at each round depend
on how much points are assigned for a single spot. We use the 
$U_{spot}$ variable to indicate this measure and $R_{total}$ to
indicate the total revenue of a single round. Equation \ref{points} shows
how the total number of points for a round are calculated.
Equation \ref{points} multiplies the total number of points that the
spots generate by two since every point is collected by two
agents. Assigning $1$ to the unit spot variable leads to a total
revenue per round of $24$, while assigning $6$ to the spot variable
leads to a $144$ total revenue.
\begin{equation}\label{points}
R_{total} = U_{spot} \times positions \times 2
\end{equation}
The aim of each agent is, hence, to be as far as possible from the
other players, in order to maximize his individual total revenue for the n rounds
played. In this paper, we describe and evaluate different algorithmic
strategies to play the Lemonade Stand Game.

\section{Algorithm design}
In this section we describe the design of the implemented
algorithms. This section is divided into two subsection, each
describing a different class of algorithms. The code of the
implemented strategies can be found in section \ref{appendix}.

\subsection{Game agnostic algorithms}
This class of algorithms includes strategies that reduce the game
to a multi-armed bandit setting. These algorithms are based on
evaluative feedback of the actions taken and do not have any internal
representation of the game or its rules. Given an array
of actions, the value of each of them, $q_*(a)$, at time step $t$ is the expected
reward $R_t$ returned when that action is taken, as equation \ref{eq2}
shows \cite{rlbook}.
\begin{equation}\label{eq2}
  q_*(a) \doteq \mathbb{E}(R_t | A_t = a)
\end{equation}
The aim of a multi-armed bandit algorithms is to maximize the total reward
by deriving an estimated value for each action at time step
$t$, $Q_t(a)$, as close as possible to the real value
$q_*(a)$. Exploration and exploitation techniques are fundamental for
this class of algorithms. The former allows to obtain more accurate
actions estimators. The latter allows to maximize the reward by
exploiting the current knowledge state.

\subsubsection{Exp3 algorithm}
The Exp3 algorithm implements an instance of the multi-armed bandit
algorithms family \cite{exp3}. Given K actions, the Exp3 algorithm keeps track of
K-length weight and probability vectors, initialized respectively at 1
and $1/K$. The probability vector assigns a probability to each action
that is calculated according to the weight given to that
action. Actions with higher weights obtain an higher probability. At
every turn the action is selected according to the probability
vector. The reward of the selected action is then used to update the
weight of that action by using the exponential distribution. The
exponential distribution increments the weights of the highest-reward
actions guaranteeing exploitation. On the other hand, exploration is
guaranteed by the gamma parameter, $\gamma \in (0, 1]$, which describes the probability to
draw a uniform random action when selecting it. The external regret, the bound
describing the difference from the optimal algorithm, of Exp3 is
limited to $O(\sqrt{KTlog(K)})$, which makes it theoretically
efficient.

\subsubsection{Epsilon greedy algorithm}
The epsilon greedy algorithm ($\epsilon$-greedy) is another type of
multi-armed bandit algorithm \cite{egreedy}. Given K actions, $\epsilon$-greedy keeps
track of frequency and rewards vectors, initialized both at 0. The
frequency vector keeps track of how many times an action has been
selected. The rewards vector contains a value estimation of the
actions. The epsilon parameter, $\epsilon \in (0, 1]$, guarantees that
the algorithm balances between exploration and exploitation. Epsilon
greedy selects a random action, exploration, with probability
$\epsilon$ and greedily selects the action with higher value with
probability $1 - \epsilon$, exploitation. The reward of the selected action is then
averaged with the frequency of that action and the previous estimated
value to update the rewards vector.


\subsection{Heuristic based algorithms}
This class of algorithms includes strategies that have an internal
representation of the game. The internal knowledge of the game is
exploited, via heuristic functions, in a strategic manner in order to
maximize the total reward.

\subsubsection{Stick and follow algorithm}
This algorithm implements a simple, and yet, effective heuristic-based
strategy. It is based on a simplified version of the ACT-R cognitive
architecture \cite{act-r}. In the initial phase of the game, the first 5
rounds, the algorithm chooses a random action (1 of the 12 available
positions) and sticks to it. This is done in order to attract another
cooperative player to select the opposite position and, as a result,
maximize both utilities. The second phase of the game strategy is
determined by an heuristic strategy. The algorithm analyzes the reward
of the last performed action and compares it to a fixed utility
threshold (implemented as 7 when $U_{spot} = 1$). This leads to the
following two cases:
\begin{enumerate}
\item \textit{Last reward $>$ threshold}: the algorithm sticks to the
  current position.
\item \textit{Last reward $\leq$ threshold}: the algorithm picks an
  opponent player randomly and sits opposite to his last position.
\end{enumerate}
The design of the heuristic function guarantees that the player does
not continue sticking in a poor reward position and limits collusion
of the other two players. In the case of the two other player
colluding, the stick and follow strategy will break the their strategy
by sitting on top of one of the two players (case number 2). 

\subsubsection{EA$^2$ algorithm}
This algorithm implements the winning strategy of the inaugural
Lemonade Stand Game Tournament \cite{ea2}. This heuristic based
strategy has at its core the analysis of the opponents' history of
game interactions in order to classify them and exploit their
behaviour. The algorithm classifies the opponents' strategies into two
generic classes: a stick and a follow strategy. The first one
describes players that stick at one or a few locations for the entire
duration of the game. The second one describes players that tend to
sit opposite to other players in order to maximize their reward. The
classification is not binary and is computed by keeping indices on the
opponents behaviour. A stick index defines how close a player is to the
ideal stick strategy, see equation \ref{eq3}. A general follow index
defines how close a player is to the ideal follow strategy, see equation \ref{eq5}. Two
additional player specific follow indices describe which of his
opponents the player is following, see equation \ref{eq4}.
\begin{equation}\label{eq3}
  s_i = - \sum_{k = 2}^{t-1}\frac{\gamma^{t-1-k}}{\Gamma}d(a_i(k), a_i(k-1))^p
\end{equation}
\begin{equation}\label{eq4}
f_{ij} = - \sum_{k = 2}^{t-1}\frac{\gamma^{t-1-k}}{\Gamma}d(a_i(k), a_j^*(k-1))^p
\end{equation}
\begin{equation}\label{eq5}
f_{i} = - \sum_{k = 2}^{t-1}\frac{\gamma^{t-1-k}}{\Gamma}\min_{j = N \setminus i}[d(a_i(k), a_j^*(k-1))]^p
\end{equation}
where $t$ represents the current time step, $\Gamma = \sum_{k =
  2}^{t-1}\gamma^{t-1-k}$, $d(a_i(k), a_j(k-1))$ represents the
smaller distance between the current position of player $i$ and
the previous position of player $j$, while $a_j^*(k-1)$ describes the
position opposite to player $j$. The $\gamma$ response parameter is
used to weight past events, like in the reinforcement learning
temporal difference method. The parameter $p$ defines the ideal type
acceptance interval, whether to include in strategy classification noisy players
that play similarly to an ideal strategy.

The indices are computed at each time step and they are needed to
select the next action. The heuristics used to select the next action
are described by the following ordered condition rules:
\begin{enumerate}
\item Player $i$ is a sticker more than player $j$. Furthermore, his
  stick index is bigger than its follow index and player $j$
  one $\Rightarrow$ sit opposite to player $i$.
\item Both opposite players have high stick index $\Rightarrow$ stick
  if utility bigger than 8 otherwise sit opposite to the player with
  higher stick index.
\item Player $i$ is a follower more than player $j$ $\Rightarrow$
  if player $i$ is following $j$ sit on player $j$ otherwise stick.
\item Players $i$ and $j$ are following each other $\Rightarrow$
  choose the opponent with highest follow index and sit on his position. 
\item Players $i$ and $j$ are sitting opposite to each other
  $\Rightarrow$ sit on the opponent with lower stick index to make it
  move.
\item None of the above applies $\Rightarrow$ stick in current position.
\end{enumerate}
The heuristic rules cover a large array of different possible
situations and strategies to exploit them and maximize the agent total
reward.

\section{Evaluation}
The implemented strategies have been extensively tested under
different scenarios with different players. We chose a game setting
consisting of 100 rounds and $U_{spot} = 6$, meaning that the total reward per round
is 144. We used the following additional strategies for testing
purposes:
\begin{itemize}
\item \textit{Random}: Strategy that at every round selects an action
following a uniform distribution.
\item \textit{Constant}: Strategy the sticks with the same action
every round. If present, the subscribed number following the constant
strategy indicates the action number.
\end{itemize}
The results were produced by iteratively running the same 100 rounds game setting
for 50 times with different random seeds and then averaging the
rewards. We can see the obtained results in table \ref{result-table}.
\begin{table}[h!]
\centering
\caption{Evaluation results of the 4 implemented strategies}
\label{result-table}
\begin{tabular}{l|l|l|l|l}
Opponents       & Exp3 \; & Egreedy \, & S\&F \;\; & EA$^2$   \\
\hline
Constant$_0$ + Constant$_0$ \;& 70.81 & 68.38   & 69.12 & 71.82 \\
Constant$_0$ + Constant$_1$ & 61.09 & 61.84   & 65.96 & 65.78 \\
Random + Random  \;   & 48.18 & 48.29   & 48.20 & 49.27 \\
Random + Constant    & 47.82 & 50.42   & 50.81 & 53.40 \\
Random + EA$^2$   & 48.03 & 52.49   & 50.85 & 52.39 \\
EA$^2$ + EA$^2$ & 40.35 & 43.89  & 48.82 & 48.21 \\
Exp3 + Egreedy  & 48.09 & 50.37   & 50.26 & 52.87 \\
S\&F + EA$^2$       & 41.59 & 42.43   & 45.67 & 46.32 \\
Constant$_0$ + Constant$_6$ & 36    & 36      & 36    & 36
\end{tabular}
\end{table}

We can observe that in an ideal setting where both opponents sit on
the same spot (Constant$_0$ + Constant$_0$) all the implemented
strategies score close to the maximum possible points (72). The
difference from the maximum score can be address to the initial
wait, for the heuristic-based algorithms, and to the exploration
phase, for the multi-armed bandit algorithms. Both classes of
algorithms perform well also against constant strategies playing
different actions. Heuristic-based algorithms internal knowledge of
the game allows them to realize the situation faster and sit opposite
to one of the two constant players without having to test all of the
possible positions. This explains their better performance in this
scenario.

Random opponents make all the algorithm perform quite
poorly and score, approximately, an average of a third of the total
points. Random strategies, in fact, results in multi-armed bandit
algorithms weighting all the actions the same and heuristic-based
algorithms not being able to exploit the opponents behaviour (random
strategy is both far from the sticker and the follower ideal strategy).

Interesting test scenarios are the ones that include, in the
opponents, at least a sticker strategy or a cooperative one (Random +
Constant and Random + EA$^2$). In these cases we can deduct that all
of the algorithms, except of the Exp3 strategy, exploit this setting
to coordinate with the right opponent. Exp3 action selection, subject
to probabilities given by the weight vector, does not allow to
continuously exploit the best position as Egreedy argmax action
selection does. We can also note that EA$^2$ player indices allow the
algorithm for faster convergence to collusion, while Egreedy and
Stick and Follow strategies are slowed down, respectively, by
exploration and random opponent selection.

Playing against two opponents that could possibly cooperate is another
important test scenario (EA$^2$ + EA$^2$ and Stick and Follow +
EA$^2$). We can observe heuristic-based strategies performing better
at avoiding the collusion of the two opponents. This is because those
strategies do not need to try different positions, once detected the
situation of the utility drop they can immediately react with the best
strategic action to break the opponents synchronization. 

Overall, from the different scenarios outlined in table
\ref{result-table}, we can infer that the heuristic-based methods
perform better than the multi-armed bandit class. Game heuristics
allow the algorithms to react to different type of situations with
fixed effective strategies that lead to reward maximization. We also
have to note that Egreedy algorithm shows a cooperative behaviour in a
different array of situations even without having an internal game
knowledge. Results also show that there is not effective strategy
against two players colluding without changing their strategy or
considering the third player.

\section{Conclusion}
In this paper we have described the design and evaluation of different
strategies to play the Lemonade Stand Game. We have analyzed both
multi-armed bandit and heuristic-based class of algorithms. The
results suggest that having internal knowledge of the game increases
the players performance. Complex heuristics lead to an even more
consistent improvement, as the EA$^2$ algorithm performing better than
all the other strategies shows.

Further research regarding multi-armed bandit algorithms for this
setting should consider weighting the actions not only based on the
obtained reward but also on the board configuration when that action
was taken in order to exploit opponents patterns. On the other hand,
the research regarding the heuristic-based algorithms should aim at
increasing the classes of the type of players in order to model and
exploit opponents that model strategies that differ from a sticker and
a follower type.


\begin{thebibliography}{8}

\bibitem{rlbook}
Sutton, R.S. and Barto, A.G., 2011. Reinforcement learning: An
introduction.

\bibitem{exp3}
Auer, P., Cesa-Bianchi, N., Freund, Y. and Schapire, R.E., 2002. The
nonstochastic multiarmed bandit problem. SIAM journal on computing,
32(1), pp.48-77.

\bibitem{egreedy}
Kuleshov, V. and Precup, D., 2014. Algorithms for multi-armed bandit
problems. arXiv preprint arXiv:1402.6028.

\bibitem{ea2}
Sykulski, A.M., Chapman, A.C., De Cote, E.M. and Jennings, N.R., 2010,
January. EA2: The Winning Strategy for the Inaugural Lemonade Stand
Game Tournament. In ECAI (pp. 209-214).

\bibitem{act-r}
Reitter, D., Juvina, I., Stocco, A. and Lebiere, C., 2010. Resistance
is futile: Winning lemonade market share through metacognitive
reasoning in a three-agent cooperative game. Proceedings of the 19th
behavior representation in modeling \& simulation (BRIMS). Charleston,
SC.
  
\end{thebibliography}

\section{Appendix A: source code}\label{appendix}
This appendix section contains the source code of the implemented algorithms.

\subsection{Exp3}
\lstinputlisting[language=Java]{../mystrategies/Exp3Strategy.java}

\subsection{Egreedy}
\lstinputlisting[language=Java]{../mystrategies/Egreedy.java}

\subsection{Stick and Follow}
\lstinputlisting[language=Java]{../mystrategies/StickAndFollow.java}

\subsection{EA$^2$}
\lstinputlisting[language=Java]{../mystrategies/EA2.java}
\lstinputlisting[language=Java]{../mystrategies/EA2Player.java}


\end{document}
