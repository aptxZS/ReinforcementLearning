% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage{listings}
\usepackage{color}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
%  frame=single,
  breaklines=true,
}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Reinforcement learning - Coursework}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Dimitri Diomaiuta - 30598109}
%
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Southampton}
%% \institute{Princeton University, Princeton NJ 08544, USA \and
%% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%% \email{lncs@springer.com}\\
%% \url{http://www.springer.com/gp/computer-science/lncs} \and
%% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
%% \begin{abstract}
%% Searching is one of the oldest artificial intelligence techniques used for problem solving. In this paper we analyze the results and scalability of both uninformed and informed search algorithms.
%% %\keywords{First keyword  \and Second keyword \and Another keyword.}
%% \end{abstract}
%
%
%

\section{Introduction}
The aim of this project is to develop algorithmic strategies to play
the Lemonade Stand Game (LSG). The LSG is a three agents game with a
12 positions setting, distributed like the hours on a clock.  At every
round, each player chooses a spot to put his stand on, $A_i = {P_1, ...,
  P_{12}}$. Every player chooses at the same time, without knowing the
choice of the other players at the current round since communication
between agents is not allowed. After each round, the utility of a
player is calculated as the sum of the distances between the closest
clockwise player and the closest anticlockwise player. The only
exception to this is when two or all players are sitting on the same
spot. In the first case the two players sitting in the same spot
collect a quarter of the total revenue of a single round while the
third collects half of it. In the second case each agent collects a
third of the total revenue. The total points given at each round depend
on how much points are assigned for a single spot. We use the 
$U_{spot}$ variable to indicate this measure and $R_{total}$ to
indicate the total revenue of a single round. Equation \ref{points} shows
how the total number of points for a round are calculated.
Equation \ref{points} multiplies the total number of points that the
spots generate by two since every point is collected by two
agents. Assigning $1$ to the unit spot variable leads to a total
revenue per round of $24$, while assigning $6$ to the spot variable
leads to a $144$ total revenue.
\begin{equation}\label{points}
R_{total} = U_{spot} \times positions \times 2
\end{equation}
The aim of each agent is, hence, to be as far as possible from the
other players, in order to maximize his individual total revenue for the n rounds
played. In this paper, we describe and evaluate different algorithmic
strategies to play the Lemonade Stand Game.

\section{Algorithm design}
In this section we describe the design of the implemented
algorithms. This section is divided into two subsection, each
describing a different class of algorithms. 

\subsection{Game agnostic algorithms}
This class of algorithms include strategies that reduce the game
to a multi-armed bandit setting. These algorithms are based on
evaluative feedback of the actions taken and do not have any internal
representation of the game or its rules. Given an array
of actions, the value of each of them, $q_*(a)$, at time step $t$ is the expected
reward $R_t$ returned when that action is taken, as equation \ref{eq2}
shows \cite{rlbook}:
\begin{equation}\label{eq2}
  q_*(a) \doteq \mathbb{E}(R_t | A_t = a)
\end{equation}
The aim of a multi-armed bandit algorithm is to derive an estimated
value for each action at time step $t$, $Q_t(a)$, as close as possible
to the real value $q_*(a)$. Exploration and exploitation techniques
are fundamental for this class of algorithms. The former allows to
obtain more accurate actions estimators. The latter allows to maximize
the reward by exploiting the current knowledge state.

\subsubsection{Exp3 algorithm}
The Exp3 algorithm implements an instance of the multi-armed bandit
algorithms family \cite{exp3}. Given K actions, the Exp3 algorithm keeps track of
K-length weight and probability vectors, initialized respectively at 1
and $1/K$. The probability vector assigns a probability to each action
that is calculated according to the weight given to that
action. Actions with higher weights obtain an higher probability. At
every turn the action is selected according to the probability
vector. The reward of the selected action is then used to update the
weight of that action by exploiting the exponential distribution. The
exponential distribution increments the weights of the highest-reward
actions guaranteeing exploitation. On the other hand, exploration is
guaranteed by the gamma parameter, $\gamma \in (0, 1]$, which describe the probability to
draw a uniform random action when selecting it. The regret, the bound
describing the difference from the optimal algorithm, of Exp3 is
limited to $O(\sqrt{KTlog(K)})$, which makes it theoretically
efficient.

\subsubsection{Epsilon greedy algorithm}
The epsilon greedy algorithm ($\epsilon$-greedy) is another type of
multi-armed bandit algorithm. Given K actions, $\epsilon$-greedy keeps
track of frequency and rewards vectors, initialized both at 0. The
frequency vector keeps track of how many times an action has been
selected. The rewards vector contains a value estimation of the
actions. The epsilon parameter, $\epsilon \in (0, 1]$, guarantees that
the algorithm balances between exploration and exploitation. Epsilon
greedy selects a random action, exploration, with probability
$\epsilon$ and greedily selects the action with higher value with
probability $1 - \epsilon$. The reward of the selected action is then
averaged with the frequency of that action and the previous estimated
value to update the rewards vector.


\subsection{Heuristic based algorithms}
\subsubsection{Stick and follow algorithm}
\subsubsection{EA2 algorithm}


\section{Evaluation}

\section{Conclusion}

\begin{thebibliography}{8}

\bibitem{rlbook}
Sutton, R.S. and Barto, A.G., 2011. Reinforcement learning: An
introduction.

\bibitem{exp3}
Auer, P., Cesa-Bianchi, N., Freund, Y. and Schapire, R.E., 2002. The
nonstochastic multiarmed bandit problem. SIAM journal on computing,
32(1), pp.48-77.

\bibitem{egreedy}
Kuleshov, V. and Precup, D., 2014. Algorithms for multi-armed bandit
problems. arXiv preprint arXiv:1402.6028.

\bibitem{ea2}
Sykulski, A.M., Chapman, A.C., De Cote, E.M. and Jennings, N.R., 2010,
January. EA2: The Winning Strategy for the Inaugural Lemonade Stand
Game Tournament. In ECAI (pp. 209-214).
  
\end{thebibliography}

\section{Appendix A: source code}\label{appendix}
This appendix section contains the source code of the implemented program.

%% \subsection{capture\_the\_flag.py}\label{capture_the_flag.py}
%% \lstinputlisting[language=Python]{../capture_the_flag.py}


\end{document}
